{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 18:05:39.258799: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-25 18:05:39.932492: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-03-25 18:05:39.932579: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-03-25 18:05:39.932585: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import tqdm\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_metric, load_dataset\n",
    "\n",
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, AutoTokenizer\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "nltk.download('punkt')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../WOT_extractive_final.xlsx')\n",
    "\n",
    "df.to_csv('temp.csv')\n",
    "df = pd.read_csv('temp.csv').rename(columns={'Unnamed: 0': 'id'}).drop(columns=['Unnamed: 0.1'])\n",
    "df['id'] = df['id'].astype(str)\n",
    "\n",
    "df['output'] = df['output'].fillna('unanswerable')\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "# CHANGE CONTEXT\n",
    "# df['input'] = df['input_context_selected']\n",
    "# df['input'] = df.apply(lambda row: f\"{row['question']}SPLITHistory: {row['history']}\", axis=1)\n",
    "df['input'] = df['context_SBERT']\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "df = df[['id', 'domain', 'labels', 'input', 'output', 'data_split', 'is_factoid', 'is_confirmation', 'is_complex', 'is_causal', 'is_listing', 'is_history', 'is_navigation', 'is_unanswerable']].fillna('')\n",
    "\n",
    "df_train = df[df['data_split'] == 'train']\n",
    "df_val = df[df['data_split'] == 'validation']\n",
    "df_test = df[df['data_split'] == 'test']\n",
    "\n",
    "ds_train = Dataset.from_pandas(df_train)\n",
    "ds_val = Dataset.from_pandas(df_val)\n",
    "ds_test = Dataset.from_pandas(df_test)\n",
    "ds_wot = DatasetDict({'train': ds_train, 'validation': ds_val, 'test': ds_test,})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and other Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model_dir = f\"Models/{model_name}/Context_SBERT/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"question: \"\n",
    "max_input_length = 512\n",
    "max_target_length = 50\n",
    "\n",
    "def clean_text(text):\n",
    "    question = nltk.sent_tokenize(text.split('SPLIT')[0])\n",
    "    sentences = nltk.sent_tokenize(text.split('SPLIT')[1])\n",
    "    text_cleaned = \"\\n \".join([\" \".join(question).lower(), \" \".join(sentences).lower()])\n",
    "    return text_cleaned\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    texts_cleaned = [clean_text(text) for text in examples[\"input\"]]\n",
    "    inputs = [prefix + text for text in texts_cleaned]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"output\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_test(examples):\n",
    "    texts_cleaned = [clean_text(text) for text in examples[\"input\"]]\n",
    "    inputs = [prefix + text for text in texts_cleaned]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True,\n",
    "                            padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return model_inputs\n",
    "\n",
    "metric = load_metric('rouge')\n",
    "exact_match_metric = load(\"exact_match\")\n",
    "\n",
    "def compute_metrics(eval_pred, eval_test = False):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels_raw = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip()))\n",
    "                      for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) \n",
    "                      for label in decoded_labels_raw]\n",
    "    decoded_labels_list = [[\"\\n\".join(nltk.sent_tokenize(label.strip()))] \n",
    "                      for label in decoded_labels_raw]\n",
    "        \n",
    "    # Compute ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "    \n",
    "    # Extract ROUGE f1 scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    #result['bleu'] = bleu.compute(predictions=decoded_preds, references=decoded_labels_list)['bleu']\n",
    "    result['exact_match'] = exact_match_metric.compute(predictions=decoded_preds, references=decoded_labels)['exact_match']\n",
    "    #result_bert_score = bert_score.compute(predictions=decoded_preds, references=decoded_labels_list, lang=\"en\")['f1']\n",
    "    #result['bert_score (avg. F1)'] = sum(result_bert_score) / len(result_bert_score)\n",
    "    \n",
    "    # Add mean generated length to metrics\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id)\n",
    "                      for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "        \n",
    "    if eval_test:\n",
    "        return {k: round(v, 4) for k, v in result.items()}, decoded_preds\n",
    "\n",
    "    with open(model_dir + 'logging.txt', 'a') as log:\n",
    "        log.write(str({k: round(v, 4) for k, v in result.items()}) + \"\\n\")\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_eval_batch_size = 64     #64\n",
    "per_device_train_batch_size = 16     #16\n",
    "gradient_accumulation_steps = 4     # per_device_train_batch_size * gradient_accumulation_steps=64 change here to be 64\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    model_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    max_steps=5000,\n",
    "    optim='adafactor',\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune\n",
    "\n",
    "free gpu memory: \n",
    "- nvidia-smi\n",
    "- sudo kill -9 PID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd7bafa7dcb47cd8e10b9b57d4929fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/QuestionAnswering/project-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79819670a8d34b0fac1709b0d49fbbd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set to wherever finetuning continues and where it is supposed \n",
    "# to be stored\n",
    "\n",
    "model_checkpoint = model_name\n",
    "model_dir = model_dir\n",
    "\n",
    "tokenized_train = ds_wot['train'].map(preprocess_data, batched=True)\n",
    "tokenized_dev = ds_wot['validation'].map(preprocess_data, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAR LOGGING.TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--t5-base/snapshots/0db7e623bcaee2daf9b859a646637ea39bf016cd/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--t5-base/snapshots/0db7e623bcaee2daf9b859a646637ea39bf016cd/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--t5-base/snapshots/0db7e623bcaee2daf9b859a646637ea39bf016cd/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--t5-base/snapshots/0db7e623bcaee2daf9b859a646637ea39bf016cd/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--t5-base/snapshots/0db7e623bcaee2daf9b859a646637ea39bf016cd/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--t5-base/snapshots/0db7e623bcaee2daf9b859a646637ea39bf016cd/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: is_history, id, __index_level_0__, is_unanswerable, input, data_split, is_listing, is_confirmation, output, is_factoid, is_causal, is_complex, is_navigation, domain. If is_history, id, __index_level_0__, is_unanswerable, input, data_split, is_listing, is_confirmation, output, is_factoid, is_causal, is_complex, is_navigation, domain are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 248\n",
      "  Num Epochs = 1250\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 5000\n",
      "  Number of trainable parameters = 222903552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   3/5000 00:01 < 1:54:24, 0.73 it/s, Epoch 0.50/1250]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: is_history, id, __index_level_0__, is_unanswerable, input, data_split, is_listing, is_confirmation, output, is_factoid, is_causal, is_complex, is_navigation, domain. If is_history, id, __index_level_0__, is_unanswerable, input, data_split, is_listing, is_confirmation, output, is_factoid, is_causal, is_complex, is_navigation, domain are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 167\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 167\n",
      "  Batch size = 64\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Saving model checkpoint to Models/t5-base/Context_SBERT/checkpoint-100\n",
      "Configuration saved in Models/t5-base/Context_SBERT/checkpoint-100/config.json\n",
      "Configuration saved in Models/t5-base/Context_SBERT/checkpoint-100/generation_config.json\n",
      "Model weights saved in Models/t5-base/Context_SBERT/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in Models/t5-base/Context_SBERT/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in Models/t5-base/Context_SBERT/checkpoint-100/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: is_history, id, __index_level_0__, is_unanswerable, input, data_split, is_listing, is_confirmation, output, is_factoid, is_causal, is_complex, is_navigation, domain. If is_history, id, __index_level_0__, is_unanswerable, input, data_split, is_listing, is_confirmation, output, is_factoid, is_causal, is_complex, is_navigation, domain are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 167\n",
      "  Batch size = 64\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Saving model checkpoint to Models/t5-base/Context_SBERT/checkpoint-200\n",
      "Configuration saved in Models/t5-base/Context_SBERT/checkpoint-200/config.json\n",
      "Configuration saved in Models/t5-base/Context_SBERT/checkpoint-200/generation_config.json\n",
      "Model weights saved in Models/t5-base/Context_SBERT/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in Models/t5-base/Context_SBERT/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in Models/t5-base/Context_SBERT/checkpoint-200/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: is_history, id, __index_level_0__, is_unanswerable, input, data_split, is_listing, is_confirmation, output, is_factoid, is_causal, is_complex, is_navigation, domain. If is_history, id, __index_level_0__, is_unanswerable, input, data_split, is_listing, is_confirmation, output, is_factoid, is_causal, is_complex, is_navigation, domain are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 167\n",
      "  Batch size = 64\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Saving model checkpoint to Models/t5-base/Context_SBERT/checkpoint-300\n",
      "Configuration saved in Models/t5-base/Context_SBERT/checkpoint-300/config.json\n",
      "Configuration saved in Models/t5-base/Context_SBERT/checkpoint-300/generation_config.json\n",
      "Model weights saved in Models/t5-base/Context_SBERT/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in Models/t5-base/Context_SBERT/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in Models/t5-base/Context_SBERT/checkpoint-300/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: is_history, id, __index_level_0__, is_unanswerable, input, data_split, is_listing, is_confirmation, output, is_factoid, is_causal, is_complex, is_navigation, domain. If is_history, id, __index_level_0__, is_unanswerable, input, data_split, is_listing, is_confirmation, output, is_factoid, is_causal, is_complex, is_navigation, domain are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 167\n",
      "  Batch size = 64\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Saving model checkpoint to Models/t5-base/Context_SBERT/checkpoint-400\n",
      "Configuration saved in Models/t5-base/Context_SBERT/checkpoint-400/config.json\n",
      "Configuration saved in Models/t5-base/Context_SBERT/checkpoint-400/generation_config.json\n",
      "Model weights saved in Models/t5-base/Context_SBERT/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in Models/t5-base/Context_SBERT/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in Models/t5-base/Context_SBERT/checkpoint-400/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: is_history, id, __index_level_0__, is_unanswerable, input, data_split, is_listing, is_confirmation, output, is_factoid, is_causal, is_complex, is_navigation, domain. If is_history, id, __index_level_0__, is_unanswerable, input, data_split, is_listing, is_confirmation, output, is_factoid, is_causal, is_complex, is_navigation, domain are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 167\n",
      "  Batch size = 64\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Saving model checkpoint to Models/t5-base/Context_SBERT/checkpoint-500\n",
      "Configuration saved in Models/t5-base/Context_SBERT/checkpoint-500/config.json\n",
      "Configuration saved in Models/t5-base/Context_SBERT/checkpoint-500/generation_config.json\n",
      "Model weights saved in Models/t5-base/Context_SBERT/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in Models/t5-base/Context_SBERT/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in Models/t5-base/Context_SBERT/checkpoint-500/special_tokens_map.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m      2\u001b[0m     model_init\u001b[39m=\u001b[39mmodel_init,\n\u001b[1;32m      3\u001b[0m     args\u001b[39m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/QuestionAnswering/project-env/lib/python3.8/site-packages/transformers/trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1540\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1541\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1542\u001b[0m )\n\u001b[0;32m-> 1543\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1544\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1545\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1546\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1547\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1548\u001b[0m )\n",
      "File \u001b[0;32m~/QuestionAnswering/project-env/lib/python3.8/site-packages/transformers/trainer.py:1791\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1789\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1790\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1791\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1794\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1795\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1796\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1797\u001b[0m ):\n\u001b[1;32m   1798\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1799\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/QuestionAnswering/project-env/lib/python3.8/site-packages/transformers/trainer.py:2539\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2536\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2538\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2539\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2542\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/QuestionAnswering/project-env/lib/python3.8/site-packages/transformers/trainer.py:2571\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2569\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2570\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2571\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2572\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2573\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2574\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/QuestionAnswering/project-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/QuestionAnswering/project-env/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1663\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1660\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[1;32m   1662\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[0;32m-> 1663\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   1664\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1665\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1666\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1667\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1668\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m   1669\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1670\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1671\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1672\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1673\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1674\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1675\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1676\u001b[0m )\n\u001b[1;32m   1678\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1680\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/QuestionAnswering/project-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/QuestionAnswering/project-env/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1097\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1094\u001b[0m             \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m v[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcuda:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(k) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_device:\n\u001b[1;32m   1095\u001b[0m                 hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(k \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[0;32m-> 1097\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfinal_layer_norm(hidden_states)\n\u001b[1;32m   1098\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m   1100\u001b[0m \u001b[39m# Add last layer\u001b[39;00m\n",
      "File \u001b[0;32m~/QuestionAnswering/project-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/QuestionAnswering/project-env/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:255\u001b[0m, in \u001b[0;36mT5LayerNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m    248\u001b[0m \n\u001b[1;32m    249\u001b[0m     \u001b[39m# T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[39m# Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[39m# w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[39m# half-precision inputs is done in fp32\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     variance \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 255\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39;49mrsqrt(variance \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvariance_epsilon)\n\u001b[1;32m    257\u001b[0m     \u001b[39m# convert into half-precision if necessary\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdtype \u001b[39min\u001b[39;00m [torch\u001b[39m.\u001b[39mfloat16, torch\u001b[39m.\u001b[39mbfloat16]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models/t5-base/Context_SBERT/checkpoint-100\n"
     ]
    }
   ],
   "source": [
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "model_checkpoint = model_dir + 'checkpoint-100'\n",
    "ds_test = ds_wot['test']\n",
    "print(model_checkpoint)\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test(examples):\n",
    "    texts_cleaned = [clean_text(text) for text in examples[\"input\"]]\n",
    "    inputs = [prefix + text for text in texts_cleaned]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True,\n",
    "                            padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file Models/t5-base/Context_SBERT/checkpoint-100/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"Models/t5-base/Context_SBERT/checkpoint-100\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file Models/t5-base/Context_SBERT/checkpoint-100/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at Models/t5-base/Context_SBERT/checkpoint-100.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file Models/t5-base/Context_SBERT/checkpoint-100/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3673ddd0f0f47a994b543ea3f00d43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "/home/ubuntu/QuestionAnswering/project-env/lib/python3.8/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "/tmp/ipykernel_57629/3234133590.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['predictions'] = predictions\n",
      "/tmp/ipykernel_57629/3234133590.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['EM'] = df_test['predictions'] == df_test['output']\n",
      "/tmp/ipykernel_57629/3234133590.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['pred_in_exp'] = df_test.apply(lambda row: len(row['predictions']) / len(row['output']) if row['predictions'] in row['output'] else 0, axis=1)\n",
      "/tmp/ipykernel_57629/3234133590.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['exp_in_pred'] = df_test.apply(lambda row: len(row['output']) / len(row['predictions']) if row['output'] in row['predictions'] else 0, axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 25.6805, 'rouge2': 13.4223, 'rougeL': 24.4023, 'rougeLsum': 24.426, 'exact_match': 0.034, 'gen_len': 9.8859}\n",
      "Prediction in Epxectation: 0.05135712630094698\n",
      "Epxectation in Prediction: 0.04383544917825948\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
    "\n",
    "test_tokenized_dataset = ds_test.map(preprocess_test, batched=True)\n",
    "\n",
    "# prepare dataloader\n",
    "test_tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "dataloader = torch.utils.data.DataLoader(test_tokenized_dataset, batch_size=64, pin_memory=True)\n",
    "\n",
    "# generate text for each batch\n",
    "all_predictions = []\n",
    "for i,batch in enumerate(dataloader):\n",
    "  batch = {k: v.to(device) for k, v in batch.items()}\n",
    "  predictions = model.generate(**batch)\n",
    "  all_predictions.append(predictions.cpu().numpy())\n",
    "\n",
    "# flatten predictions\n",
    "all_predictions_flattened = [pred for preds in all_predictions for pred in preds]\n",
    "\n",
    "# tokenize and pad titles\n",
    "all_titles = tokenizer(test_tokenized_dataset[\"output\"], max_length=max_target_length,\n",
    "                       truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
    "\n",
    "# compute metrics\n",
    "predictions_labels = [all_predictions_flattened, all_titles]\n",
    "scores, predictions = compute_metrics(predictions_labels, eval_test=True)\n",
    "\n",
    "df_test['predictions'] = predictions\n",
    "\n",
    "df_test['EM'] = df_test['predictions'] == df_test['output']\n",
    "df_test['pred_in_exp'] = df_test.apply(lambda row: len(row['predictions']) / len(row['output']) if row['predictions'] in row['output'] else 0, axis=1)\n",
    "df_test['exp_in_pred'] = df_test.apply(lambda row: len(row['output']) / len(row['predictions']) if row['output'] in row['predictions'] else 0, axis=1)\n",
    "\n",
    "print(scores)\n",
    "print(f\"Prediction in Epxectation: {df_test['pred_in_exp'].mean()}\")\n",
    "print(f\"Epxectation in Prediction: {df_test['exp_in_pred'].mean()}\")\n",
    "\n",
    "df_test.to_excel(model_dir + 'test_set_predictions.xlsx')\n",
    "\n",
    "with open(model_dir + 'test_evaluation.txt', 'a') as f:\n",
    "  f.write('Class|elements in class|EM|elements in cooking|EM cooking|elements in diy|EM diy\\n')\n",
    "  for q_class in ['is_factoid', 'is_confirmation', 'is_complex', 'is_causal', 'is_listing', 'is_history', 'is_navigation']:\n",
    "    df_t_cooking = df_test[(df_test[q_class] == True) & (df_test['domain']=='cooking')] \n",
    "    df_t_diy = df_test[(df_test[q_class] == True) & (df_test['domain']=='diy')]\n",
    "    df_t = df_test[df_test[q_class] == True]\n",
    "    f.write(f'{q_class}|{len(df_t)}|{df_t[\"EM\"].sum()/len(df_t)}|{len(df_t_cooking)}|{df_t_cooking[\"EM\"].sum()/len(df_t_cooking)}|{len(df_t_diy)}|{df_t_diy[\"EM\"].sum()/len(df_t_diy)}\\n')\n",
    "  f.write('\\n\\n')\n",
    "  \n",
    "  f.write('Class|elements in class|Pred_in_Exp|elements in cooking|Pred_in_Exp cooking|elements in diy|Pred_in_Exp diy\\n')\n",
    "  for q_class in ['is_factoid', 'is_confirmation', 'is_complex', 'is_causal', 'is_listing', 'is_history', 'is_navigation']:\n",
    "    df_t_cooking = df_test[(df_test[q_class] == True) & (df_test['domain']=='cooking')] \n",
    "    df_t_diy = df_test[(df_test[q_class] == True) & (df_test['domain']=='diy')]\n",
    "    df_t = df_test[df_test[q_class] == True]\n",
    "    f.write(f'{q_class}|{len(df_t)}|{df_t[\"pred_in_exp\"].mean()}|{len(df_t_cooking)}|{df_t_cooking[\"pred_in_exp\"].mean()}|{len(df_t_diy)}|{df_t_diy[\"pred_in_exp\"].mean()}\\n')\n",
    "  f.write('\\n\\n')\n",
    "  \n",
    "  f.write('Class|elements in class|Exp_in_Pred|elements in cooking|Exp_in_Pred cooking|elements in diy|Exp_in_Pred diy\\n')\n",
    "  for q_class in ['is_factoid', 'is_confirmation', 'is_complex', 'is_causal', 'is_listing', 'is_history', 'is_navigation']:\n",
    "    df_t_cooking = df_test[(df_test[q_class] == True) & (df_test['domain']=='cooking')] \n",
    "    df_t_diy = df_test[(df_test[q_class] == True) & (df_test['domain']=='diy')]\n",
    "    df_t = df_test[df_test[q_class] == True]\n",
    "    f.write(f'{q_class}|{len(df_t)}|{df_t[\"exp_in_pred\"].mean()}|{len(df_t_cooking)}|{df_t_cooking[\"exp_in_pred\"].mean()}|{len(df_t_diy)}|{df_t_diy[\"exp_in_pred\"].mean()}\\n')\n",
    "  f.write('\\n\\n')\n",
    "  \n",
    "  f.write(str(df_test.groupby('domain').sum().reset_index()[['domain', 'EM']]))\n",
    "  \n",
    "\n",
    "# plot EM over training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(model_dir + \"logging.txt\", \"r\") as file:\n",
    "    contents = file.read()\n",
    "\n",
    "list_of_dicts = []\n",
    "for line in contents.split(\"\\n\"):\n",
    "    if line.strip() != \"\":\n",
    "        dict_str = line.strip()\n",
    "        dict_obj = eval(dict_str)\n",
    "        list_of_dicts.append(dict_obj)\n",
    "    \n",
    "list_of_EM = [d['exact_match'] for d in list_of_dicts]\n",
    "epochs = [(i+1) * 100 for i in range(len(list_of_EM))]\n",
    "\n",
    "plt.plot(epochs, list_of_EM)\n",
    "plt.ylim(ymax = 1.1 * max(list_of_EM), ymin = 0)\n",
    "plt.title('Validation Loss over Training Steps')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Exact Match (Validation Set)')\n",
    "plt.show()\n",
    "plt.savefig(model_dir + 'Training_Validation_Loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
