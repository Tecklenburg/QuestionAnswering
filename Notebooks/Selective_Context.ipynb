{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:50:21.865176: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-26 15:50:22.543802: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-03-26 15:50:22.543883: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-03-26 15:50:22.543889: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from transformers import T5Tokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../WOT_extractive_original.xlsx').drop(columns=['Unnamed: 0']) \n",
    "df['input'] = df['input'].apply(lambda x: str(x).replace('|', '| ')) \n",
    "#df['input'] = df['input'].apply(lambda x: str(x).replace('\\n', '\\n '))\n",
    "df['input'] = df['input'].apply(lambda x: str(x).replace(';', '; ')) \n",
    "df['output'] = df['output'].apply(lambda x: str(x).replace('|', '| ')) \n",
    "df['output'] = df['output'].apply(lambda x: str(x).replace('\\\\n', '\\n')) \n",
    "df['output'] = df['output'].apply(lambda x: str(x).replace(';', '; ')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use a dry cloth to soak up any liquid from the vomi\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[674, 'output'])\n",
    "df.loc[674, 'output'] = 'use a dry cloth to soak up any liquid from the vomit'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perfect Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "model_name = \"allenai/unifiedqa-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def compare(a, b):\n",
    "    for i in range(len(b)):\n",
    "        if a[i] != b[i]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def get_trunc_instructions(instructions, answer, available_tokens):\n",
    "    if compare(answer, [148, 33, 612]):\n",
    "        if len(instructions) < available_tokens:\n",
    "            return instructions\n",
    "        return instructions[-available_tokens:]\n",
    "    if compare(answer, [4273]):\n",
    "        if len(instructions) < available_tokens:\n",
    "            return instructions\n",
    "        return instructions[-available_tokens:]\n",
    "    if compare(answer, [150]):\n",
    "        if len(instructions) < available_tokens:\n",
    "            return instructions\n",
    "        return instructions[-available_tokens:]\n",
    "    if compare(answer, [766, 8, 2912, 19, 344, 9455, 11, 3, 4013, 3, 1956, 89, 41, 2884, 11, 944, 3, 1956, 75, 61]):\n",
    "        answer = [766, 8, 2912, 19, 344, 9455, 11, 3, 4013, 3, 1956, 89, 41, 2884, 11, 944, 3, 1956, 75, 137]\n",
    "    if compare(answer, [314, 716, 41, 127, 1200, 6, 3, 99, 25, 114, 61]):\n",
    "        answer = [314, 716, 41, 127, 1200, 6, 3, 99, 25, 114, 201]\n",
    "    if compare(answer, [3, 99, 338, 5726, 3, 11049, 3321, 7, 6, 18874, 16, 387]):\n",
    "        answer = [41, 99, 338, 5726, 3, 11049, 3321, 7, 6, 18874, 16, 387]\n",
    "    if compare(answer, [3, 20987, 371, 3224, 2912]):\n",
    "        answer = [41, 20987, 371, 3224, 2912]\n",
    "    if compare(answer, [6273, 3380, 49, 342, 21, 44, 709, 505, 716, 41, 127, 95, 12, 204, 477, 61]):\n",
    "        answer = [6273, 3380, 49, 342, 21, 44, 709, 505, 716, 41, 127, 95, 12, 204, 477, 137]\n",
    "    \n",
    "    try:\n",
    "        answer = answer[:-1]\n",
    "        position = [(i, i+len(answer)) for i in range(len(instructions)) if compare(instructions[i:i+len(answer)],answer)]\n",
    "        start = position[0][0]\n",
    "        end = position[0][1]\n",
    "    except:\n",
    "        print(tokenizer.decode(answer))\n",
    "        print(tokenizer.decode(instructions))\n",
    "        print(answer)\n",
    "        print(instructions)\n",
    "        \n",
    "    \n",
    "    before = True\n",
    "    \n",
    "    while end-start < available_tokens:\n",
    "        if before:\n",
    "            before = False\n",
    "            start -= 10\n",
    "        else:\n",
    "            before = True\n",
    "            end += 10\n",
    "            \n",
    "        start = max(start, 0)\n",
    "        end = min(len(instructions), end)\n",
    "        \n",
    "        if start==0 and end ==len(instructions):\n",
    "            return instructions[start:end]\n",
    "    \n",
    "    while end-start < available_tokens:\n",
    "        if before:\n",
    "            before = False\n",
    "            start -= 1\n",
    "        else:\n",
    "            before = True\n",
    "            end += 1\n",
    "            \n",
    "        start = max(start, 0)\n",
    "        end = min(len(instructions), end)\n",
    "        \n",
    "        if start==0 and end ==len(instructions):\n",
    "            return instructions[start:end]\n",
    "        \n",
    "    return instructions[start:end]\n",
    "        \n",
    "    \n",
    "\n",
    "df['tokens_taken'] = df.apply(lambda row: len(tokenizer.encode(f\"{row['question']}SPLITInstructions: \\n\\nHistory: {row['history']})\")), axis=1)\n",
    "df['encoded_instructions'] = df.apply(lambda row: tokenizer.encode(row['input'][len(f\"{row['question']}SPLITInstructions: \") : len(row['input']) - 2 - len(f\"\\n\\nHistory: {row['history']})\")]), axis=1)\n",
    "df['trunc_instructions'] = df.apply(lambda row: get_trunc_instructions(row['encoded_instructions'], tokenizer.encode(row['output']), 512 - row['tokens_taken']), axis=1)\n",
    "df['trunc_instructions'] = df['trunc_instructions'].apply(lambda x: tokenizer.decode(x[:-1]))\n",
    "df['input_context_selected'] = df.apply(lambda row: f\"{row['question']}SPLITInstructions:{row['trunc_instructions']}\\nHistory:{row['history']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('../WOT_extractive_final.xlsx')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBERT Retrieval\n",
    "https://www.sbert.net/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2').to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['steps'] = df['steps'].apply(lambda x: x[2:-2].split(\"\\', \\'\"))\n",
    "df['steps_embeddings'] = df['steps'].apply(model.encode)\n",
    "df['question_embedding'] = df['question'].apply(lambda x: model.encode([x])[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## consine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['relevant_step_ind'] = df.apply(lambda row: util.cos_sim(row['question_embedding'], row['steps_embeddings']).argmax(), axis=1)\n",
    "df['relevant_step'] = df.apply(lambda row: row['steps'][row['relevant_step_ind']], axis=1)\n",
    "df['context_SBERT'] = df.apply(lambda row: f\"{row['question']}SPLITInstructions:\\n|Title:\\n{row['title']}\\n|Ingredients:\\n{row['ingredients']}\\n|Step:\\n{row['relevant_step']}\\n|History:\\n{row['history']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('../WOT_extractive_final.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vary number of Utterances in conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cooking = pd.read_json('https://wizard-of-tasks.s3.us-west-2.amazonaws.com/wizard_of_tasks_cooking_v1.0.json').transpose()\n",
    "df_diy = pd.read_json('https://wizard-of-tasks.s3.us-west-2.amazonaws.com/wizard_of_tasks_diy_v1.0.json').transpose()\n",
    "\n",
    "df_hist = pd.concat([df_cooking, df_diy])\n",
    "\n",
    "def get_history(conversation, question, number_of_utterances):\n",
    "    conv = df_hist.loc[conversation, 'turns']\n",
    "    for i, el in enumerate([el['text'] for el in conv]):\n",
    "        if el == question:\n",
    "            ind = i\n",
    "            break\n",
    "    \n",
    "    out = [f\"{el['role']}: {el['text']}\" for el in conv[max(0, ind-number_of_utterances):ind]]\n",
    "\n",
    "    return ' | '.join(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['history_6'] = df.apply(lambda row: get_history(row['conversation'], row['question'], 6), axis=1)\n",
    "df['tokens_taken_6'] = df.apply(lambda row: len(tokenizer.encode(f\"{row['question']}SPLITInstructions: \\n\\nHistory: {row['history_6']})\")), axis=1)\n",
    "df['encoded_instructions_6'] = df.apply(lambda row: tokenizer.encode(row['input'][len(f\"{row['question']}SPLITInstructions: \") : len(row['input']) - 1 - len(f\"\\n\\nHistory: {row['history']})\")]), axis=1)\n",
    "df['trunc_instructions_6'] = df.apply(lambda row: get_trunc_instructions(row['encoded_instructions_6'], tokenizer.encode(row['output']), 512 - row['tokens_taken_6']), axis=1)\n",
    "df['trunc_instructions_6'] = df['trunc_instructions_6'].apply(lambda x: tokenizer.decode(x[:-1]))\n",
    "\n",
    "df['input_context_selected_0_history'] = df.apply(lambda row: f\"{row['question']}SPLITInstructions:{row['trunc_instructions_6']}\", axis=1)\n",
    "\n",
    "df['history_2'] = df.apply(lambda row: get_history(row['conversation'], row['question'], 2), axis=1)\n",
    "df['input_context_selected_2_history'] = df.apply(lambda row: f\"{row['question']}SPLITInstructions:{row['trunc_instructions_6']}\\nHistory:{row['history_2']}\", axis=1)\n",
    "\n",
    "df['history_4'] = df.apply(lambda row: get_history(row['conversation'], row['question'], 4), axis=1)\n",
    "df['input_context_selected_4_history'] = df.apply(lambda row: f\"{row['question']}SPLITInstructions:{row['trunc_instructions_6']}\\nHistory:{row['history_4']}\", axis=1)\n",
    "\n",
    "\n",
    "df['input_context_selected_6_history'] = df.apply(lambda row: f\"{row['question']}SPLITInstructions:{row['trunc_instructions_6']}\\nHistory:{row['history_6']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just to clarify did you mean to say I should add red pepper flakes?SPLITInstructions:a little heat from red pepper flakes. | Ingredients: 1 pound dried rigatoni pasta 1/4 teaspoon fine sea salt 3 tablespoons extra-virgin olive oil 1 large yellow onion, thinly sliced 3 cloves garlic, finely chopped 1/4 teaspoon ground black pepper 1/4 teaspoon crushed red chile flakes 1/2 cup raw walnut halves and pieces 1 (28.0-ounce) can whole tomatoes, drained 2 (6.0-ounce) cans tuna packed in water, drained 3/4 cup seedless raisins 1/2 cup fresh flat-leaf parsley, chopped | Steps: cook pasta in a 6-quart pot of boiling salted water according to package directions until al dente. meanwhile, heat oil in a large skillet over high heat until hot but not smoking. add onions, garlic, salt, pepper and red pepper flakes and cook, stirring occasionally, until golden, 6 to 8 minutes. add walnuts and cook for 2 to 3 minutes, until lightly toasted. add tomatoes, breaking them up with a spoon, and simmer for 5 minutes. stir in tuna and raisins. when pasta is done, drain well and return to pot. add tuna mixture to pasta in pot, toss gently and warm over low heat for about 30 seconds. garnish with parsley and serve immediately.\n",
      "History:student: What does el dente mean? | teacher: Al dente literally means \"to the tooth\". In terms of cooking, you want the pasta to have a little resistance when you bite into it and it shouldn't be completely limp or mushy. | student: Oh ok! Well I just started the water boiling for the rigatoni, that will probably take 5-8 minutes just to start boiling. Is there anything i can do while I'm waiting for the water to boil? | teacher: Yes, while you are boiling water you should proceed to the next step which is to heat oil in a large skillet over high heat, but not smoking. | student: Gotcha.  Once the water is boiling and the oil is heated, what's next? | teacher: Next you will add the onions, garlic, salt, pepper, and rep pepper flakes to the skillet and cook for about 6-8 minutes. \n"
     ]
    }
   ],
   "source": [
    "print(df.loc[12, 'input_context_selected_6_history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('../WOT_extractive_final.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
