{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 09:49:44.749029: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-22 09:49:45.382285: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-03-22 09:49:45.382379: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-03-22 09:49:45.382385: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import T5Tokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../WOT_extractive_original.xlsx').drop(columns=['Unnamed: 0']) \n",
    "df['input'] = df['input'].apply(lambda x: str(x).replace('|', '| ')) \n",
    "#df['input'] = df['input'].apply(lambda x: str(x).replace('\\n', '\\n '))\n",
    "df['input'] = df['input'].apply(lambda x: str(x).replace(';', '; ')) \n",
    "df['output'] = df['output'].apply(lambda x: str(x).replace('|', '| ')) \n",
    "df['output'] = df['output'].apply(lambda x: str(x).replace('\\\\n', '\\n')) \n",
    "df['output'] = df['output'].apply(lambda x: str(x).replace(';', '; ')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use a dry cloth to soak up any liquid from the vomi\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[674, 'output'])\n",
    "df.loc[674, 'output'] = 'use a dry cloth to soak up any liquid from the vomit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "model_name = \"allenai/unifiedqa-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def compare(a, b):\n",
    "    for i in range(len(b)):\n",
    "        if a[i] != b[i]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def get_trunc_instructions(instructions, answer, available_tokens):\n",
    "    if compare(answer, [148, 33, 612]):\n",
    "        if len(instructions) < available_tokens:\n",
    "            return instructions\n",
    "        return instructions[-available_tokens:]\n",
    "    if compare(answer, [4273]):\n",
    "        if len(instructions) < available_tokens:\n",
    "            return instructions\n",
    "        return instructions[-available_tokens:]\n",
    "    if compare(answer, [150]):\n",
    "        if len(instructions) < available_tokens:\n",
    "            return instructions\n",
    "        return instructions[-available_tokens:]\n",
    "    if compare(answer, [766, 8, 2912, 19, 344, 9455, 11, 3, 4013, 3, 1956, 89, 41, 2884, 11, 944, 3, 1956, 75, 61]):\n",
    "        answer = [766, 8, 2912, 19, 344, 9455, 11, 3, 4013, 3, 1956, 89, 41, 2884, 11, 944, 3, 1956, 75, 137]\n",
    "    if compare(answer, [314, 716, 41, 127, 1200, 6, 3, 99, 25, 114, 61]):\n",
    "        answer = [314, 716, 41, 127, 1200, 6, 3, 99, 25, 114, 201]\n",
    "    if compare(answer, [3, 99, 338, 5726, 3, 11049, 3321, 7, 6, 18874, 16, 387]):\n",
    "        answer = [41, 99, 338, 5726, 3, 11049, 3321, 7, 6, 18874, 16, 387]\n",
    "    if compare(answer, [3, 20987, 371, 3224, 2912]):\n",
    "        answer = [41, 20987, 371, 3224, 2912]\n",
    "    if compare(answer, [6273, 3380, 49, 342, 21, 44, 709, 505, 716, 41, 127, 95, 12, 204, 477, 61]):\n",
    "        answer = [6273, 3380, 49, 342, 21, 44, 709, 505, 716, 41, 127, 95, 12, 204, 477, 137]\n",
    "    \n",
    "    try:\n",
    "        answer = answer[:-1]\n",
    "        position = [(i, i+len(answer)) for i in range(len(instructions)) if compare(instructions[i:i+len(answer)],answer)]\n",
    "        start = position[0][0]\n",
    "        end = position[0][1]\n",
    "    except:\n",
    "        print(tokenizer.decode(answer))\n",
    "        print(tokenizer.decode(instructions))\n",
    "        print(answer)\n",
    "        print(instructions)\n",
    "        \n",
    "    \n",
    "    before = True\n",
    "    \n",
    "    while end-start < available_tokens:\n",
    "        if before:\n",
    "            before = False\n",
    "            start -= 10\n",
    "        else:\n",
    "            before = True\n",
    "            end += 10\n",
    "            \n",
    "        start = max(start, 0)\n",
    "        end = min(len(instructions), end)\n",
    "        \n",
    "        if start==0 and end ==len(instructions):\n",
    "            return instructions[start:end]\n",
    "    \n",
    "    while end-start < available_tokens:\n",
    "        if before:\n",
    "            before = False\n",
    "            start -= 1\n",
    "        else:\n",
    "            before = True\n",
    "            end += 1\n",
    "            \n",
    "        start = max(start, 0)\n",
    "        end = min(len(instructions), end)\n",
    "        \n",
    "        if start==0 and end ==len(instructions):\n",
    "            return instructions[start:end]\n",
    "        \n",
    "    return instructions[start:end]\n",
    "        \n",
    "    \n",
    "\n",
    "df['tokens_taken'] = df.apply(lambda row: len(tokenizer.encode(f\"{row['question']}SPLITInstructions: \\n\\nHistory: {row['history']})\")), axis=1)\n",
    "df['encoded_instructions'] = df.apply(lambda row: tokenizer.encode(row['input'][len(f\"{row['question']}SPLITInstructions: \") : len(row['input']) - len(f\"\\n\\nHistory: {row['history']})\")]), axis=1)\n",
    "df['trunc_instructions'] = df.apply(lambda row: get_trunc_instructions(row['encoded_instructions'], tokenizer.encode(row['output']), 512 - row['tokens_taken']), axis=1)\n",
    "df['trunc_instructions'] = df['trunc_instructions'].apply(lambda x: tokenizer.decode(x))\n",
    "df['input_context_selected'] = df.apply(lambda row: f\"{row['question']}SPLITInstructions:{row['trunc_instructions']}\\n \\n History: {row['history']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('../WOT_extractive_final.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
